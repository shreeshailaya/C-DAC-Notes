# Introduction to PyTorch

## What is PyTorch?

PyTorch is an open-source machine learning library developed by Facebook's AI Research lab (FAIR). It's primarily used for applications such as computer vision and natural language processing. PyTorch is known for:

- Dynamic computational graph (define-by-run)
- Pythonic programming style
- Strong GPU acceleration support
- Deep integration with Python
- Rich ecosystem of tools and libraries

## Key Features

1. **Dynamic Computation Graph**: Unlike static frameworks, PyTorch builds the graph on-the-fly as operations are executed
2. **Automatic Differentiation**: Enables gradient-based optimization methods
3. **GPU Acceleration**: Seamless computation on CUDA-enabled GPUs
4. **Pythonic Interface**: Natural coding style familiar to Python developers
5. **Extensibility**: Easy to extend and customize

## Installing PyTorch

```python
# Using pip
pip install torch torchvision torchaudio

# Using conda
conda install pytorch torchvision torchaudio -c pytorch
```

## PyTorch Ecosystem

- **torchvision**: Computer vision datasets, models, and transforms
- **torchaudio**: Audio processing tools and datasets
- **torchtext**: Natural language processing utilities and datasets
- **torchserve**: Model serving framework
- **PyTorch Lightning**: High-level interface for organizing PyTorch code

---

# Torch Data Structures

PyTorch's fundamental data structure is the **Tensor**, similar to NumPy's ndarray but with GPU compatibility and automatic differentiation capabilities.

## 1. Tensors

Tensors are multi-dimensional arrays that serve as the basic building block for all computations in PyTorch.

### Creating Tensors

```python
import torch

# From Python lists
x = torch.tensor([1, 2, 3, 4])
print(f"Tensor from list: {x}")

# From NumPy array
import numpy as np
np_array = np.array([1, 2, 3, 4])
x = torch.from_numpy(np_array)
print(f"Tensor from NumPy: {x}")

# Pre-defined tensors
zeros = torch.zeros(3, 4)  # 3x4 tensor of zeros
ones = torch.ones(2, 3, 4)  # 2x3x4 tensor of ones
rand = torch.rand(2, 3)  # 2x3 tensor of random numbers (uniform [0,1])
randn = torch.randn(3, 4)  # 3x4 tensor of random numbers (normal distribution)
eye = torch.eye(3)  # 3x3 identity matrix
arange = torch.arange(0, 10, step=2)  # tensor([0, 2, 4, 6, 8])
linspace = torch.linspace(0, 10, steps=5)  # tensor([0.0, 2.5, 5.0, 7.5, 10.0])

print(f"Zeros: \n{zeros}\n")
print(f"Random: \n{rand}\n")
print(f"Identity: \n{eye}\n")
```

### Tensor Attributes

```python
x = torch.randn(3, 4, 5)

# Shape and dimensions
print(f"Shape: {x.shape}")
print(f"Dimensions: {x.dim()}")
print(f"Size: {x.size()}")  # Same as shape

# Data types
print(f"Data type: {x.dtype}")

# Device (CPU/GPU)
print(f"Device: {x.device}")

# Number of elements
print(f"Element count: {x.numel()}")
```

### Tensor Operations

```python
# Arithmetic operations
a = torch.tensor([1, 2, 3])
b = torch.tensor([4, 5, 6])

print(f"a + b = {a + b}")  # Element-wise addition
print(f"a - b = {a - b}")  # Element-wise subtraction
print(f"a * b = {a * b}")  # Element-wise multiplication
print(f"a / b = {a / b}")  # Element-wise division

# Matrix multiplication
m1 = torch.tensor([[1, 2], [3, 4]])
m2 = torch.tensor([[5, 6], [7, 8]])
print(f"Matrix multiplication (matmul): \n{torch.matmul(m1, m2)}\n")
print(f"Matrix multiplication (@): \n{m1 @ m2}\n")

# In-place operations (modify the tensor directly)
x = torch.tensor([1, 2, 3])
print(f"Original x: {x}")
x.add_(5)  # Note the underscore suffix indicating in-place
print(f"After x.add_(5): {x}")
```

### Indexing and Slicing

```python
x = torch.tensor([[1, 2, 3, 4], 
                 [5, 6, 7, 8], 
                 [9, 10, 11, 12]])

# Basic indexing
print(f"First row: {x[0]}")
print(f"Element at position (1,2): {x[1, 2]}")

# Slicing
print(f"First two rows: \n{x[:2]}\n")
print(f"First and third columns: \n{x[:, [0, 2]]}\n")
print(f"Sub-matrix: \n{x[1:3, 1:3]}\n")

# Advanced indexing
indices = torch.tensor([0, 2])
print(f"Selected rows: \n{x[indices]}\n")

# Boolean masking
mask = x > 6
print(f"Mask (x > 6): \n{mask}\n")
print(f"Elements where x > 6: {x[mask]}")
```

### Reshaping Tensors

```python
x = torch.tensor([[1, 2, 3, 4], 
                 [5, 6, 7, 8], 
                 [9, 10, 11, 12]])

# View (shares the same memory)
y = x.view(4, 3)
print(f"Reshaped with view: \n{y}\n")

# Reshape (may copy data)
z = x.reshape(2, 6)
print(f"Reshaped with reshape: \n{z}\n")

# Transpose
x_t = x.t()  # or x.transpose(0, 1)
print(f"Transposed: \n{x_t}\n")

# Permute (generalized transpose)
a = torch.randn(2, 3, 4)
b = a.permute(2, 0, 1)  # Permute dimensions from [2,3,4] to [4,2,3]
print(f"Original shape: {a.shape}")
print(f"After permute: {b.shape}")

# Adding/removing dimensions
x = torch.tensor([1, 2, 3, 4])
x_unsqueezed = x.unsqueeze(1)  # Add dimension at index 1
print(f"After unsqueeze: {x_unsqueezed.shape}")

y = torch.zeros(3, 1, 4, 1)
y_squeezed = y.squeeze()  # Remove all dimensions of size 1
print(f"After squeeze: {y_squeezed.shape}")
```

### Type Casting

```python
x = torch.tensor([1, 2, 3], dtype=torch.int32)
print(f"Original tensor: {x}, dtype: {x.dtype}")

# Convert to float
x_float = x.float()  # or x.to(torch.float32)
print(f"Float tensor: {x_float}, dtype: {x_float.dtype}")

# Convert to double (float64)
x_double = x.double()  # or x.to(torch.float64)
print(f"Double tensor: {x_double}, dtype: {x_double.dtype}")

# Other type conversions
x_long = x.long()     # to int64
x_bool = x.bool()     # to boolean
```

### Device Management (CPU/GPU)

```python
# Create tensor on CPU
x = torch.tensor([1, 2, 3])
print(f"Default device: {x.device}")

# Move to GPU (if available)
if torch.cuda.is_available():
    device = torch.device("cuda")
    x_gpu = x.to(device)  # or x.cuda()
    print(f"Moved to: {x_gpu.device}")
    
    # Move back to CPU
    x_cpu = x_gpu.cpu()
    print(f"Moved back to: {x_cpu.device}")
else:
    print("CUDA not available")
```

## 2. Autograd: Automatic Differentiation

PyTorch's automatic differentiation system is called autograd. It tracks operations on tensors and computes gradients automatically.

```python
# Create tensors with gradient tracking
x = torch.tensor([2.0], requires_grad=True)
y = torch.tensor([3.0], requires_grad=True)

# Perform operations
z = x**2 + y**3

# Compute gradients
z.backward()

# Access gradients
print(f"dz/dx: {x.grad}")  # Should be 2*x = 4
print(f"dz/dy: {y.grad}")  # Should be 3*y^2 = 27
```

More complex example:

```python
# Create a tensor with requires_grad=True to track computation
x = torch.ones(2, 2, requires_grad=True)
print(f"x: \n{x}\n")

# Do some operations
y = x + 2
print(f"y = x + 2: \n{y}\n")

z = y * y * 3
print(f"z = 3 * y * y: \n{z}\n")

out = z.mean()
print(f"out = z.mean(): {out}\n")

# Backpropagation
out.backward()

# Print gradients
print(f"x.grad: \n{x.grad}\n")
```

### Using no_grad for Inference

```python
x = torch.tensor([2.0], requires_grad=True)

# Normal computation with gradient tracking
y1 = x ** 2
print(f"Grad enabled: {y1.requires_grad}")

# Temporarily disable gradient tracking
with torch.no_grad():
    y2 = x ** 2
    print(f"Grad disabled: {y2.requires_grad}")
```

## 3. Neural Networks with nn Module

PyTorch provides the `nn` module to build neural networks easily.

### Linear Layer Example

```python
import torch.nn as nn

# Define a simple neural network
class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(10, 20)  # Input dim: 10, Output dim: 20
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(20, 5)   # Input dim: 20, Output dim: 5
        
    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

# Create the model
model = SimpleNN()
print(model)

# Sample input
input_tensor = torch.randn(32, 10)  # Batch size: 32, Feature dim: 10

# Forward pass
output = model(input_tensor)
print(f"Output shape: {output.shape}")  # Should be [32, 5]
```

### Common Neural Network Layers

```python
# Linear (Fully connected) layer
linear = nn.Linear(in_features=10, out_features=20)

# Convolutional layers
conv1d = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)
conv2d = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)
conv3d = nn.Conv3d(in_channels=1, out_channels=8, kernel_size=3, stride=1, padding=1)

# Pooling layers
maxpool2d = nn.MaxPool2d(kernel_size=2, stride=2)
avgpool2d = nn.AvgPool2d(kernel_size=2, stride=2)

# Normalization layers
batchnorm1d = nn.BatchNorm1d(num_features=10)
batchnorm2d = nn.BatchNorm2d(num_features=16)

# Recurrent layers
rnn = nn.RNN(input_size=10, hidden_size=20, num_layers=2)
lstm = nn.LSTM(input_size=10, hidden_size=20, num_layers=2)
gru = nn.GRU(input_size=10, hidden_size=20, num_layers=2)

# Dropout (regularization)
dropout = nn.Dropout(p=0.5)

# Embedding layer
embedding = nn.Embedding(num_embeddings=10000, embedding_dim=300)
```

## 4. Loss Functions and Optimizers

PyTorch provides common loss functions and optimizers for training neural networks.

### Loss Functions

```python
import torch.nn.functional as F

# Sample data
predictions = torch.randn(3, 5)  # 3 samples, 5 classes (logits)
targets = torch.tensor([1, 0, 4])  # Class indices

# Cross entropy loss (for classification)
ce_loss = F.cross_entropy(predictions, targets)
print(f"Cross entropy loss: {ce_loss.item()}")

# Mean squared error (for regression)
regression_pred = torch.tensor([0.5, 1.5, 2.5])
regression_targets = torch.tensor([1.0, 2.0, 3.0])
mse_loss = F.mse_loss(regression_pred, regression_targets)
print(f"MSE loss: {mse_loss.item()}")

# Binary cross entropy (for binary classification)
binary_pred = torch.sigmoid(torch.randn(4))
binary_targets = torch.tensor([0.0, 1.0, 1.0, 0.0])
bce_loss = F.binary_cross_entropy(binary_pred, binary_targets)
print(f"BCE loss: {bce_loss.item()}")
```

### Optimizers

```python
import torch.optim as optim

# Define a simple model
model = nn.Linear(10, 1)

# Different optimizers
sgd = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
adam = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))
rmsprop = optim.RMSprop(model.parameters(), lr=0.01, alpha=0.99)
adagrad = optim.Adagrad(model.parameters(), lr=0.01)

# Simple training loop example
def train_step(model, optimizer, x, y):
    # Reset gradients
    optimizer.zero_grad()
    
    # Forward pass
    y_pred = model(x)
    loss = F.mse_loss(y_pred, y)
    
    # Backward pass
    loss.backward()
    
    # Update weights
    optimizer.step()
    
    return loss.item()

# Sample data
x = torch.randn(64, 10)
y = torch.randn(64, 1)

# Train for 10 steps
for step in range(10):
    loss = train_step(model, adam, x, y)
    print(f"Step {step+1}, Loss: {loss:.4f}")
```

## 5. Working with Data

PyTorch provides utilities for data loading and preprocessing through `torch.utils.data`.

### Dataset and DataLoader

```python
from torch.utils.data import Dataset, DataLoader

# Custom dataset example
class CustomDataset(Dataset):
    def __init__(self, data, targets):
        self.data = data
        self.targets = targets
        
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        return self.data[idx], self.targets[idx]

# Create sample data
data = torch.randn(100, 10)  # 100 samples with 10 features each
targets = torch.randint(0, 2, (100,))  # Binary targets

# Create dataset and dataloader
dataset = CustomDataset(data, targets)
dataloader = DataLoader(dataset, batch_size=16, shuffle=True)

# Iterate through batches
for batch_idx, (batch_data, batch_targets) in enumerate(dataloader):
    print(f"Batch {batch_idx+1}:")
    print(f"  Data shape: {batch_data.shape}")
    print(f"  Targets shape: {batch_targets.shape}")
    
    if batch_idx >= 2:  # Only show first 3 batches
        break
```

## 6. Saving and Loading Models

PyTorch provides methods to save and load model parameters and entire models.

```python
# Define a simple model
model = nn.Linear(10, 5)

# Save just the model parameters (recommended way)
torch.save(model.state_dict(), "model_params.pth")

# Load the parameters into a model
new_model = nn.Linear(10, 5)
new_model.load_state_dict(torch.load("model_params.pth"))

# Save the entire model (alternative approach)
torch.save(model, "full_model.pth")

# Load the entire model
loaded_model = torch.load("full_model.pth")
```

## 7. Practical Example: Training a Simple Neural Network

```python
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset

# Create synthetic data
X_train = torch.randn(1000, 10)
y_train = torch.randint(0, 3, (1000,))  # 3 classes

# Create dataset and dataloader
train_dataset = TensorDataset(X_train, y_train)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)

# Define the model
class SimpleClassifier(nn.Module):
    def __init__(self):
        super(SimpleClassifier, self).__init__()
        self.fc1 = nn.Linear(10, 32)
        self.fc2 = nn.Linear(32, 16)
        self.fc3 = nn.Linear(16, 3)  # 3 output classes
        
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# Initialize model, loss, and optimizer
model = SimpleClassifier()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training loop
num_epochs = 10
for epoch in range(num_epochs):
    running_loss = 0.0
    correct = 0
    total = 0
    
    for inputs, labels in train_loader:
        # Zero the parameter gradients
        optimizer.zero_grad()
        
        # Forward pass
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        
        # Backward pass and optimize
        loss.backward()
        optimizer.step()
        
        # Track statistics
        running_loss += loss.item()
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
    
    # Print epoch statistics
    epoch_loss = running_loss / len(train_loader)
    epoch_acc = 100 * correct / total
    print(f"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%")

print("Training finished!")
```

## 8. Using PyTorch with GPU

```python
# Check if GPU is available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Create a model and move it to GPU
model = SimpleClassifier().to(device)

# Move data to GPU
inputs = torch.randn(64, 10).to(device)
labels = torch.randint(0, 3, (64,)).to(device)

# Forward pass on GPU
outputs = model(inputs)
loss = criterion(outputs, labels)

# Backward pass on GPU
loss.backward()

# Move results back to CPU if needed
cpu_outputs = outputs.cpu()
```

## Summary

- **Tensors** are the fundamental building blocks in PyTorch
- **Autograd** provides automatic differentiation for gradient-based optimization
- **nn Module** simplifies building neural network architectures
- **Optimizers** implement various optimization algorithms
- **DataLoader** efficiently handles data loading and batching
- PyTorch supports seamless **GPU acceleration**
- Models can be easily **saved and loaded**

PyTorch's intuitive design and flexibility make it a popular choice for researchers and practitioners in deep learning and AI.
